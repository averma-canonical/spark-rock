name: spark
summary: Spark ROCK
description: Spark ROCK
license: Apache-2.0

version: "0.1"
base: ubuntu:22.04
platforms:
  amd64:

entrypoint: ["bash", "/opt/entrypoint.sh"]

env:
  - SPARK_HOME: /opt/spark
  - JAVA_HOME: /usr/lib/jvm/java-11-openjdk-amd64
  - PYTHONPATH: /opt/spark/python:/opt/spark/python/build:/opt/spark/bin:$SNAP/usr/lib/python3/dist-packages:$PYTHONPATH

parts:

  spark:
    plugin: dump
    source: https://downloads.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz
    source-checksum: sha512/1e8234d0c1d2ab4462d6b0dfe5b54f2851dcd883378e0ed756140e10adfb5be4123961b521140f580e364c239872ea5a9f813a20b73c69cb6d4e95da2575c29c
    overlay-script: |
      set -ex
      sed -i 's/http:\/\/deb.\(.*\)/https:\/\/deb.\1/g' /etc/apt/sources.list
      apt-get update
      ln -svf /lib /lib64
      # apt-get install -y bash tini libc6 libpam-modules krb5-user libnss3 procps
      apt-get install -y bash
      mkdir -p /opt/spark/python
      touch /opt/spark/RELEASE
      rm /bin/sh
      ln -svf /bin/bash /bin/sh
      echo "auth required pam_wheel.so use_uid" >> /etc/pam.d/su
      # chgrp root /etc/passwd && chmod ug+rw /etc/passwd
      rm -rf /var/cache/apt/*
    organize:
      jars: opt/spark/jars
      bin: opt/spark/bin
      sbin: opt/spark/sbin
      python: opt/spark/python
      kubernetes/dockerfiles/spark/entrypoint.sh: opt/entrypoint.sh
      kubernetes/dockerfiles/spark/decom.sh: opt/decom.sh
      examples: opt/spark/examples
      kubernetes/tests: opt/spark/tests
      data: opt/spark/data
    stage:
       - opt/spark/jars
       - opt/spark/bin
       - opt/spark/sbin
       - opt/entrypoint.sh
       - opt/decom.sh
       - opt/spark/examples
       - opt/spark/tests
       - opt/spark/data
       - opt/spark/python

  hadoop-jars:
    plugin: nil
    after: [spark]
    build-packages:
      - wget
    overlay-script: |
      AWS_JAVA_SDK_BUNDLE_VERSION='1.11.874'
      HADOOP_AWS_VERSION='3.2.2'
      mkdir -p $CRAFT_PART_INSTALL/opt/spark/jars
      cd $CRAFT_PART_INSTALL/opt/spark/jars
      wget -q "https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/${AWS_JAVA_SDK_BUNDLE_VERSION}/aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar"
      wget -q "https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/${AWS_JAVA_SDK_BUNDLE_VERSION}/aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar.sha1"  
      echo "`cat aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar.sha1`  aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar" | sha1sum --check
      if  [[ $? -ne 0 ]]
        then
          echo "DOWNLOAD ERROR: aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar could not be downloaded properly! Exiting...."
          exit 1
      fi
      wget -q "https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${HADOOP_AWS_VERSION}/hadoop-aws-${HADOOP_AWS_VERSION}.jar"
      wget -q "https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${HADOOP_AWS_VERSION}/hadoop-aws-${HADOOP_AWS_VERSION}.jar.sha1"
      echo "`cat hadoop-aws-${HADOOP_AWS_VERSION}.jar.sha1`  hadoop-aws-${HADOOP_AWS_VERSION}.jar" | sha1sum --check
      if  [[ $? -ne 0 ]]
        then
          echo "DOWNLOAD ERROR: hadoop-aws-${HADOOP_AWS_VERSION}.jar could not be downloaded properly! Exiting...."
          exit 1
      fi
    stage:
      - opt/spark/jars

  user-setup:
    plugin: nil
    after: [hadoop-jars]
    overlay-packages:
      - tini
      - libc6
      - libpam-modules
      - krb5-user
      - libnss3
      - procps
      - openjdk-11-jre-headless
      - python3-setuptools
    overlay-script: |
      SPARK_HOME=/opt/spark
      # chmod a+x /opt/decom.sh
      
      # Create a user in the $CRAFT_OVERLAY chroot
      SPARK_UID=185
      useradd -R $CRAFT_OVERLAY -M -r -u 1000 -U spark
